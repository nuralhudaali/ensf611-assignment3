{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (37 total marks)\n",
    "### Due: October 24 at 11:59pm\n",
    "\n",
    "### Name: Nur-Alhuda Ali"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete\n",
    "X, y = load_concrete()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fdc93a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using negative mean square error scoring:\n",
      "\n",
      "    Training Accuracy  Validation Accuracy\n",
      "DT          47.279761            73.447331\n",
      "RF          29.576135            45.052441\n",
      "GB           3.379440            22.819636\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "## STEP 3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)   # use default test size\n",
    "\n",
    "DT_model = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "RF_model = RandomForestRegressor(max_depth=5, random_state=0)  # max_features left as default, which is max_features=\"auto\", and \"auto\" means max_features = n_features, which is what we want for regression\n",
    "GB_model = GradientBoostingRegressor(max_depth=5, random_state=0) # n_estimators and learning_rate left as default\n",
    "\n",
    "DT_model.fit(X_train, y_train)\n",
    "RF_model.fit(X_train, y_train)\n",
    "GB_model.fit(X_train, y_train)\n",
    "\n",
    "## STEP 4\n",
    "\n",
    "# For the Decision Tree model:\n",
    "DT_cv = cross_validate(DT_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "DT_train_score = np.mean(DT_cv['train_score']) * -1\n",
    "DT_test_score = np.mean(DT_cv['test_score']) * -1\n",
    "scores_MSE = { 'DT' : [DT_train_score, DT_test_score] } # add results to dictionary of negative MSE scores\n",
    "\n",
    "# For the Random Forest model:\n",
    "RF_cv = cross_validate(RF_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "RF_train_score = np.mean(RF_cv['train_score']) * -1\n",
    "RF_test_score = np.mean(RF_cv['test_score']) * -1\n",
    "scores_MSE['RF'] = [RF_train_score, RF_test_score]\n",
    "\n",
    "# For the Gradient Boosting model:\n",
    "GB_cv = cross_validate(GB_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "GB_train_score = np.mean(GB_cv['train_score']) * -1\n",
    "GB_test_score = np.mean(GB_cv['test_score']) * -1\n",
    "scores_MSE['GB'] = [GB_train_score, GB_test_score]\n",
    "\n",
    "## STEP 5\n",
    "# Visualize the results.\n",
    "print(\"Using negative mean square error scoring:\\n\")\n",
    "results = pd.DataFrame.from_dict(scores_MSE, orient='index', columns=['Training Accuracy', 'Validation Accuracy'])\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31715a9d",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "83539f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using R2 scoring:\n",
      "\n",
      "    Training Accuracy  Validation Accuracy\n",
      "DT           0.834465             0.738697\n",
      "RF           0.896561             0.840951\n",
      "GB           0.988171             0.919348\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "# For the Decision Tree model:\n",
    "DT_cv = cross_validate(DT_model, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
    "DT_train_score = np.mean(DT_cv['train_score'])\n",
    "DT_test_score = np.mean(DT_cv['test_score'])\n",
    "scores_R2 = { 'DT' : [DT_train_score, DT_test_score] } # add results to dictionary of negative MSE scores\n",
    "\n",
    "# For the Random Forest model:\n",
    "RF_cv = cross_validate(RF_model, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
    "RF_train_score = np.mean(RF_cv['train_score'])\n",
    "RF_test_score = np.mean(RF_cv['test_score'])\n",
    "scores_R2['RF'] = [RF_train_score, RF_test_score]\n",
    "\n",
    "# For the Gradient Boosting model:\n",
    "GB_cv = cross_validate(GB_model, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
    "GB_train_score = np.mean(GB_cv['train_score'])\n",
    "GB_test_score = np.mean(GB_cv['test_score'])\n",
    "scores_R2['GB'] = [GB_train_score, GB_test_score]\n",
    "\n",
    "# Visualize the results.\n",
    "print(\"Using R2 scoring:\\n\")\n",
    "results = pd.DataFrame.from_dict(scores_R2, orient='index', columns=['Training Accuracy', 'Validation Accuracy'])\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "2. Out of the models you tested, which model would you select for this dataset and why?\n",
    "3. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "1. Using a linear model for this data in the previous assignment, the training and validation accuracies using R2 scoring were 0.61 and 0.62, respectively, and using MSE, were 111. 36 and 95.90, respectively. All the models used in the assignment performed significantly better than the linear model.\n",
    "\n",
    "    Using mean square error, the decision tree model has training and validation accuracies of 47.28 and 73.45, which are 64.08 and 22.45 lower than those of the linear model, respectively. The random forest model has training and validation mean square errors of 29.58 and 45.05, which are 81.78 and 50.85 lower than those of the linear model, respectively. Again, the gradient-boosting model was the best-performing with training and validation mean square errors of 3.38 and 22.82, which are 107.98 and 73.08, respectively.\n",
    "\n",
    "    Using R2 scoring, the decision tree model has training and validation accuracies of 0.83 and 0.74, which are 0.22 and 0.12 higher than those of the linear model, respectively. The random forest model has training and validation R2 scores of 0.90 and 0.84, which are 0.29 and 0.22 higher than those of the linear model, respectively. The best-performing model is the gradient boosting regressor. It has training and validation R2 scores of 0.99 and 0.92, which are 0.38 and 0.30 higher than those of the linear model, respectively.\n",
    "\n",
    "2. Out of the models tested, I would select the gradient boosting model since it is the best-performing. It has the lowest training and validation mean square errors, indicating that it makes the highest quality predictions (has the lowest prediction error) of all the models. As expected, its training and validation R2 scores are also the highest of all the models, and are just generally very high, being very close to 1, indicating that it is the best fit for the data.\n",
    "\n",
    "3. Both the decistion tree and random forest models did not perform as well as the gradient boosting model.\n",
    "\n",
    "    For the decision tree model, I would play around with the maximum depth of the tree to see if increasing or descreasing it would increase its accuracy. For all the models, the maximum depth was set to 5. Since pre-pruning is the main strategy used to tune a decision tree model, altering the maximum depth will greatly impact the model's accuracy. Whereas the other models have other parameters that their performance relies on, like number of estimators in random forests or learning rate in gradient boosting machines, the decision tree model only has a few, one of them being maximum depth of the tree.\n",
    "\n",
    "    To increase the accuracy of the random forest model, I would add more trees, increasing the n_estimators parameter, to better accound for the behaviour of the data and make more accurate predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\\\n",
    "\\\n",
    "I used the ENSF 611 Jupyter notebooks as a guideline for how to use SciKit-Learn for cross_validate(). Other than that, I relied on the SciKit-Learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) to help me understand what the different parameters for cross_validate() were, what values I should put for them, and what the function returns in order to be able to properly extract the training and testing scores from the return value. I completed all the steps in order because every step relies on the one before it. However, after completing all the steps, I did go back to tweak certain parameters to see how that would affect my results in order to better understand what was going on.\n",
    "\n",
    "I generally don't like to use generative AI to help me code as I prefer to do more manual research and read the documentation for libraries that I am using myself - so I did not use any generative AI for this process.\n",
    "\n",
    "A challenge I had was determining whether to use the entire dataset or just the training set for cross-validation. After doing my own research, I learned that using the training set for cross-validation is best practice done to ensure that the model does not overfit the data and provides a more accuracte reflection of how it will perform on unseen data - which was eventually confirmed by our instructor.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (17.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X:  (178, 13) \n",
      "Type(s) of X:\n",
      " Alcohol                         float64\n",
      "Malic acid                      float64\n",
      "Ash                             float64\n",
      "Alcalinity of ash               float64\n",
      "Magnesium                         int64\n",
      "Total phenols                   float64\n",
      "Flavanoids                      float64\n",
      "Nonflavanoid phenols            float64\n",
      "Proanthocyanins                 float64\n",
      "Color intensity                 float64\n",
      "Hue                             float64\n",
      "OD280/OD315 of diluted wines    float64\n",
      "Proline                           int64\n",
      "dtype: object\n",
      "\n",
      "Size of y:  (178,) \n",
      "Type of y: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import wine dataset\n",
    "\n",
    "wine_df = pd.read_csv('wine.data', names=['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols',\n",
    "                                          'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'], index_col=False)\n",
    "\n",
    "# The dataset has 14 columns. The first column is for the wine class, which is the target variable that we want to classify wine samples into.\n",
    "# The feature matrix X will contain all columns other than 'Class'. Class will be made the target vector y.\n",
    "X = wine_df.drop(columns='Class')\n",
    "y = wine_df['Class']\n",
    "\n",
    "# Print size and type of X and y.\n",
    "print(\"Size of X: \", X.shape, \"\\nType(s) of X:\\n\", X.dtypes)\n",
    "print(\"\\nSize of y: \", y.shape, \"\\nType of y:\", y.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a28af110",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ea266921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "display(wine_df.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "834fc8fe",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "97c6e9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class                           0\n",
       "Alcohol                         0\n",
       "Malic acid                      0\n",
       "Ash                             0\n",
       "Alcalinity of ash               0\n",
       "Magnesium                       0\n",
       "Total phenols                   0\n",
       "Flavanoids                      0\n",
       "Nonflavanoid phenols            0\n",
       "Proanthocyanins                 0\n",
       "Color intensity                 0\n",
       "Hue                             0\n",
       "OD280/OD315 of diluted wines    0\n",
       "Proline                         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "display(wine_df.isnull().sum()) # There are no missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "070956af",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b37a6fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples of Each Type of Wine\n",
      "Class 1:  59\n",
      "Class 2:  71\n",
      "Class 3:  48\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "mask = wine_df['Class'] == 1\n",
    "num_class1_samples = len(wine_df[mask])\n",
    "\n",
    "mask = wine_df['Class'] == 2\n",
    "num_class2_samples = len(wine_df[mask])\n",
    "\n",
    "mask = wine_df['Class'] == 3\n",
    "num_class3_samples = len(wine_df[mask])\n",
    "\n",
    "print(\"Number of Samples of Each Type of Wine\")\n",
    "print(\"Class 1: \", num_class1_samples)\n",
    "print(\"Class 2: \", num_class2_samples)\n",
    "print(\"Class 3: \", num_class3_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Data Size  Training Accuracy  Validation Accuracy\n",
      "DT   (178, 13)           0.994357             0.894017\n",
      "SVC  (178, 13)           0.680427             0.676638\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "## STEP 3\n",
    "dtc_model = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "svc_model = SVC(random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # use default test size\n",
    "\n",
    "dtc_model.fit(X_train, y_train)\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "## STEP 4\n",
    "\n",
    "# For the Decision Tree Classifier\n",
    "dtc_cv = cross_validate(dtc_model, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "dtc_train_score = np.mean(dtc_cv['train_score'])\n",
    "dtc_test_score = np.mean(dtc_cv['test_score'])\n",
    "scores_ACC = { 'DT' : [X.shape, dtc_train_score, dtc_test_score] }\n",
    "\n",
    "# For the Support Vector Classifier\n",
    "svc_cv = cross_validate(svc_model, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "svc_train_score = np.mean(svc_cv['train_score'])\n",
    "svc_test_score = np.mean(svc_cv['test_score'])\n",
    "scores_ACC['SVC'] = [X.shape, svc_train_score, svc_test_score]\n",
    "\n",
    "## STEP 5\n",
    "results = pd.DataFrame.from_dict(scores_ACC, orient='index', columns=['Data Size', 'Training Accuracy', 'Validation Accuracy'])\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2e17878",
   "metadata": {},
   "source": [
    "#### Step 5.2: Visualize Classification Errors\n",
    "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "44b091a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  2,  0],\n",
       "       [ 0, 20,  1],\n",
       "       [ 0,  0,  8]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Implement best model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# The method that gave the highest accuracy is the Decisiion Tree Classifier. So this is the model we will use for the confusion matrix.\n",
    "y_pred = dtc_model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "09d21b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(145.22222222222229, 0.5, 'True Value')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: Print confusion matrix using a heatmap\n",
    "sns.heatmap(conf_matrix, xticklabels=['Type 1', 'Type 2', 'Type 3'], yticklabels=['Type 1', 'Type 2', 'Type 3'], square=True, annot=True, cbar=True)\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.ylabel('True Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5ef95947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Type 1       1.00      0.88      0.93        16\n",
      "      Type 2       0.91      0.95      0.93        21\n",
      "      Type 3       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['Type 1', 'Type 2', 'Type 3'])\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
    "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
    "1. How many samples were incorrectly classified in step 5.2? \n",
    "1. In this case, is maximizing precision or recall more important? Why?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "\n",
    "1. The training and validation accuracies for the decision tree model were 0.99 and 0.89, respectively, whereas for the support vector machines model, they were 0.680 and 0.677, respectively. The decision tree model accuracies were much higher than the SVC model, indicating that the former is a much better fit for the data in their current state. The SVC model has low training and validation accuracies that are both almost the same, which means this model has high bias.\n",
    "\n",
    "2. One reason the support vector machines model did not work as well as the decision tree model is because it has many parameters that were left as default. For example, important parameters such as the regularization parameter C, the choice of the kernel, and the kernel-specific parameters, like gamma for the Gaussian kernel, were not specified. Tweaking these parameters could drastically improve the SVC model's performance.\n",
    "\n",
    "    Another reason for the model's bad performance could be because support vector machines require careful preprocessing of the data, like handling class imbalances, removing outliers, etc.\n",
    "\n",
    "3. Using the high-performing decision tree model, only 3 samples were incorrectly classified. 2 samples were predicted as Type 2, but were actually Type 1 wines. 1 sample was predicted as Type 3, but was actually a Type 2 wine. Of the 45 samples in the test set, all but 3 were predicted correctly (true positives).\n",
    "\n",
    "4. Whether precision or recall is more imoprtant depends on the application of the model. For example, if Type 1 is the most expensive wine and Type 3 is the least expensive, then from a business owner's perspective, a false negative for the Type 1 wine category would be more costly than a false positive. Misclassifying an expensive wine as a less expensive wine results in more profit loss to the business than classifying a cheap wine as an expensive one. So for the Type 1 category, maximizing recall is more important. On the other hand, for the cheapest, Type 3 wine, a false negative would result in increased profits (although unethical), whereas a false positive for Type 3 would result in losses (more expensive wines classified as the cheapest) - so, in this case, precision is more important.\n",
    "\n",
    "    In conclusion, in multi-classification, it is important to assess each category/class individually (or at least the most costly ones) for whether maximizing precision or recall is more important. Once this assessment is done, the model can be adjusted to meet the precision or recall goals for each class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\n",
    "\\\n",
    "I used the ENSF 611 Jupyter notebooks as a guideline for how to use SciKit-Learn for the confusion matrix and classification report. Other than that, I relied on the SciKit-Learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to help me understand what the parameters of the confusion matrix function do. Just like in the first part of this assignment, I completed all the steps in order because every step relies on the one before it.\n",
    "\n",
    "A challenge I had was figuring out whether maximizing precision or recall was more important. Since this is a multi-classification problem, it is not as simple to determine which is more important like in the binary case. I did try to use generative AI to help me answer this by asking it \"What is more important in a multi-classification problem - precision or recall? What is more costly, false negatives or false positives?\" However, it wasn't of much help since the answer it gave me was that because multi-classification problems are complex, it depends on the application. So from there, I started thinking of different scenarios in which this wine classification model might be used and for each scenario, I tried to discern whether precision or recall was more important, which led me to my answer above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "\n",
    "In *Part 1: Regression*, I found that for complex datasets, such as the concrete one used, that are high-dimensional, involve many features and intricate interactions between them, and have the potential for a lot of noise in the data (often a problem with concrete properties), gradient boosting models perform better than random forest and single decision tree models for a multitude of reasons. When dealing with high-dimensional datasets, gradient boosting models are able to assess which features are the most important to the model and ignores or minimizes other features, reducing overfitting. In addition to being able to better-handle datasets with many features, gradient boosting models are also able to discern the complex relationships between models, all the while, ignoring outliers in the data, due to their iterative nature where each subsequent tree created learns from the errors of the one before it. This is reflected in the results of the first part of this assignment where the gradient boosting model had training and validation R2 scores of 0.99 and 0.92 compared to the lower scores for the decision tree model, 0.83 and 0.74, and the random forest model, 0.90 and 0.84, respectively.\n",
    "\n",
    "However, when comparing these 3 non-linear models to the linear regression model used for the same dataset in the previous assignment, all 3 non-linear models far outperform the linear one, which had training and validation R2 scores of 0.61 and 0.62, respectively. This is likely due to the high number of features in this dataset and the complex interactions between the features that the linear model is unable to capture.\n",
    "\n",
    "In *Part 2: Classification*, I found that the decision tree model performed much better than the support vector machines model, with training and validation accuracies of 0.99 and 0.89, compared to the SVM's, which were 0.680 and 0.677, respectively. This was likely due to the fact that support vector machines are much more complex, requiring a lot of fine-tuning of parameters and pre-processing to have a well-performing model. This highlights the importance of starting with basic models and assessing their performance for the given dataset before moving to a more complex, tedious model. I also observed that with multi-classification problems, both precision and recall are important because they can mean the same thing and have the same implications from class to class. So for these problems, the goal should be to look at what is important for the specific application of the model and strike a balance between both precision and recall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "I disliked how redundant/repetitvie the observations/interpretation part of the assignment was. I felt like I had already interpreted the results and included data to justify my findings in the questions following each coding part of the assignment.\n",
    "\n",
    "I found it interesting creating the confusion matrix and interpreting it. Seeing an actual visual depiction of the results rather than them being a table and having them broken down in a confusion matrix allowed me to deepen my understanding of the model's performance and what precision and recall mean."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa21e53b",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (3 marks)\n",
    "\n",
    "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
    "\n",
    "Is `LinearSVC` a good fit for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30fea72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aabc68a4",
   "metadata": {},
   "source": [
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
